import sys
import time
from pyspark.sql import *
from pyspark.sql import SparkSession
import datetime
from pyspark import SparkContext
import io
from functions import *
import getpass
import os
import subprocess

########################################  MAIN - Test Integration ###########################################
if __name__ == '__main__':

    #after the upgrade this is required to call hdfs from the command line
    username = getpass.getuser()
    
    # if username.lower()[1] == 'u':
    os.environ["HADOOP_CONF_DIR"]="/etc/hadoop/conf"
    spark = (SparkSession.builder.appName('Build Waiver Table').enableHiveSupport().getOrCreate())
    sc = SparkContext.getOrCreate()
    spark.conf.set("spark.sql.debug.maxToStringFields", 5000)
    spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")
    spark.conf.set("spark.sql.hive.convertMetastoreOrc","false")
    # reading Global param , generated by Xldeploy dictionary on /etc/Globalparams.conf 
    props={}
    props=ReadingGlobalParams()

    tmpODAT = sys.argv[3]
    props = apply_activation_odat_j1(props , tmpODAT ) 

    # Get query from file
    props['query_file_path'] = './etc/QueryDefinitions.csv'
    props, tables, apps, schemas = Read_table_and_code_indicators(props)

    ################### Main code ###################

    # Clean query and create df with it and convert it in csv and put it in .wait folder

    # Create df with query
    df = create_df_with_query(props, spark, apps, schemas)

    #Write df to CSV file in hdfs
    props, folder_with_files_hdfs = create_csv_from_df_in_hdfs(props, df)

    #Copy the file to machine .wait folder from hdfs
    put_csv_in_folder(props, folder_with_files_hdfs)

    ################### Main code ###################