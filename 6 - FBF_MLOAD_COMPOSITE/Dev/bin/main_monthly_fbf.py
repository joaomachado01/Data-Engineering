import sys
import time
from pyspark.sql import *
from pyspark.sql import SparkSession
import datetime
#from dateutil.relativedelta import relativedelta, SU
from pyspark import SparkContext
import io
from functions_monthly_fbf import *
from Audit_trail import *
import getpass
import os

########################################  MAIN - Test Integration ###########################################
if __name__ == '__main__':
    #after the upgrade this is required to call hdfs from the command line
    username = getpass.getuser()
    # if username.lower()[1] == 'u':
    os.environ["HADOOP_CONF_DIR"]="/etc/hadoop/conf"
    spark = (SparkSession.builder.appName('Build Monthly Partition').enableHiveSupport().getOrCreate())
    sc = SparkContext.getOrCreate()
    spark.conf.set("spark.sql.debug.maxToStringFields", 5000)
    spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict")
    spark.conf.set("spark.sql.hive.convertMetastoreOrc","false")
    #sc.addPyFile("bin/Functions.py")
    #sc.addPyFile("bin/Audit_trail.py")
    # reading Global param , generated by Xldeploy dictionary on /etc/Globalparams.conf 
    props={}
    props=ReadingGlobalParams()
    #Add to props the name of the current Job.
    if username.lower()[1] == 'u':
        props['cod_job']="UFBFMLOADCOMPOSITE"
    elif username.lower()[1] == 'b' :
        props['cod_job']="BFBFMLOADCOMPOSITE"
    elif username.lower()[1] == 'p':
        props['cod_job']="PFBFMLOADCOMPOSITE"
    print(props)
    props["monthly_indicators_file"]="./etc/tablesToLoad.txt"
    props["hive_db"]=props["hive_db_name_dest"]
    props["code_appli_dir"]=props["codeAppforDir"]
    #props["history_audit_tabel_name"]='tbl_audit_trail'
    
    #props['TrueNameOfPartkey']='partition_key'
    #props['Path_hdfs_db']="/dr/fbf/public/hive/fbf_credit_alert.db"

    tmpODAT = sys.argv[3]
    odate= tmpODAT
    delta_months=1
    last_day_previous_month=get_last_day_previous_month(odate,delta_months)
    last_work_day=get_last_work_day(last_day_previous_month)
    props['history_audit_tabel_name_Final']=props["hive_db"]+'.'+props["history_audit_tabel_name"]
    props["monthly_partition"]=last_work_day+'0001' 
    props["yearmonth"]=last_work_day[0:6]

    # Create last composite Key_partition variable
    delta_months=2
    last_day_second_previous_month=get_last_day_previous_month(odate,delta_months)
    last_work_day_second_previous_month=get_last_work_day(last_day_second_previous_month)
    props["secondlastyearmonth"]=last_work_day_second_previous_month+'0001'
    print("### Last Composite Key_partition ###")
    print(props["secondlastyearmonth"])

    props,tables,select_str=Read_table_and_code_indicators(props)
    audit_schema = get_schema_empty_table_audit(spark)
    max_col_id = get_max_column_ID( spark, props['history_audit_tabel_name_Final'])
    props['ODAT']=props["monthly_partition"]
    #Insert 'Start_Job' in Hive Table
    table = tables[0]
    props["HiveTabel"] = table
    start_time   =  time.time()
    dicoTableMain = construct_tmp_values_for_audit_trail(start_time, None, props)
    max_col_id=write_audit_row_management( spark  , props , dicoTableMain  ,  'start_job' , audit_schema, max_col_id )



    for i,table in enumerate(tables):
        props["hivetable"] = table
        props["HiveTabel"] = table
        props['OnlyTable'] = table.split('.')[1]
        props["columns_str"]=get_columns_as_str(spark,props)
        props["columns_str2"]=props["columns_str"]+','
        props["columns_str3"]=props["columns_str"]+',' 
        props["columns_str_with_no_overage"]=props["columns_str2"].replace("primary_table.nb_is_overage_start,","").replace("primary_table.is_overage,","")
        props["columns_str_with_no_overagestart"]=props["columns_str3"].replace("primary_table.overage_start,","")
        print("### COLUNAS WITH NO OVERAGE ###") 
        print(props["columns_str_with_no_overage"]) 
        props["codeindicators"]=props[table+"_ind"]
        
        dicoTable = construct_tmp_values_for_audit_trail( start_time , None , props  )
        max_col_id = write_audit_row_management( spark  , props , dicoTable  ,  'start' , audit_schema, max_col_id )


        max_col_id = load_working(spark, select_str[i], props, dicoTable  , audit_schema, max_col_id )
        max_col_id = load_final_table(spark, props, dicoTable  , audit_schema, max_col_id )

        max_col_id = write_audit_row_management( spark  , props , dicoTable  ,  'finish' , audit_schema, max_col_id )

        # select_str_tab=select_str_for_insert(select_str[i],props)
        # # insert_str=Read_insert_parameters(select_str_tab,props)
        # insert_str, drop_str=Read_insert_parameters_alt(select_str_tab,props)
        # spark.sql(drop_str)
        # print(insert_str)
        # spark.sql(insert_str)
    #Insert 'Finish_Job' in Hive Table
    table = tables[len(tables)-1]
    props["HiveTabel"] = table
    dicoTableMain['cod_file'] = props['HiveTabel']
    max_col_id = write_audit_row_management( spark  , props , dicoTableMain  ,  'finish_job' , audit_schema, max_col_id  )